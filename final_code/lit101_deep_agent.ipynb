{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-03T22:14:05.305698287Z",
     "start_time": "2024-10-03T22:14:04.932928437Z"
    }
   },
   "source": [
    "#!pip install gym\n",
    "#!pip install tensorflow[and-cuda]\n",
    "#!pip install pygame"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T22:14:05.305968757Z",
     "start_time": "2024-10-03T22:14:04.965063265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# set tf warning level to 2 ....shows errors but not warnings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "#tf.__version__"
   ],
   "id": "72861eebc814b36f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-03T22:14:06.685367376Z",
     "start_time": "2024-10-03T22:14:04.965267881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gym import Env\n",
    "from gym.spaces import MultiBinary, Discrete, Box\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "#tf.config.run_functions_eagerly(True)\n",
    "#print(tf.config.functions_run_eagerly())\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import load_model, Sequential\n",
    "from tensorflow.keras import layers, models\n",
    "# for experience replay\n",
    "from collections import deque\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "# tyr legacy adam due to numpy error\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "import pygame\n",
    "\n",
    "\n",
    "# get component class from custom module\n",
    "from component_module import Component, plot_correlation_network, plot_component_comparison, getKDEDensity, getDistributionBins, kde_mutual_information, drop_static_columns, performFourierAndLimitHarmonics, scaler_sec_midnight, save_component, load_component, calculate_phase_and_time_difference\n",
    "\n",
    "# Set the random seed\n",
    "seed_value = 44\n",
    "np.random.seed(seed_value)\n",
    "random.seed(seed_value)"
   ],
   "id": "89483b04081c5e6e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-03T22:14:06.685823757Z",
     "start_time": "2024-10-03T22:14:06.644193131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if TensorFlow is built with CUDA support\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"CUDA is available\")\n",
    "    print(f\"Device: {tf.config.list_physical_devices('GPU')[0]}\")\n",
    "else:\n",
    "    print(\"CUDA is not available\")"
   ],
   "id": "66dae882324f6dbe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n",
      "Device: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1727993646.600932  101554 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1727993646.641760  101554 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1727993646.642032  101554 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Create Environment"
   ],
   "id": "8b45b612b25fa240"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T22:14:06.790586913Z",
     "start_time": "2024-10-03T22:14:06.647679610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ubuntu\n",
    "#df_normalised = pd.read_csv(\"/home/loz/Documents/GitHub/MSc-Project/SWaT_12_23_sec_minmax.csv\", index_col='time_in_seconds')\n",
    "#df_normalised = pd.read_csv(\"/home/loz/MSc/MSc_Project/SWaT_12_23_sec_minmax.csv\", index_col='time_in_seconds')\n",
    "\n",
    "df_normalised = pd.read_csv(\"/home/loz/MSc/MSc_Project/SWaT_sec_minmax_22_comps.csv\", index_col='time_in_seconds')\n",
    "\n",
    "data_array = df_normalised.to_numpy()\n",
    "#df_normalised.shape"
   ],
   "id": "7f882fb888e5454b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T22:14:06.818827322Z",
     "start_time": "2024-10-03T22:14:06.789686324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#df_normalised\n",
    "data_array.shape"
   ],
   "id": "4270862575c1a5ca",
   "outputs": [
    {
     "data": {
      "text/plain": "(86400, 22)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T22:14:06.865437895Z",
     "start_time": "2024-10-03T22:14:06.792513378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#data_array[35].max()"
   ],
   "id": "7f9b7fd3fbd6dbc6",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [],
   "id": "2e77426240355b11"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T22:14:06.871261855Z",
     "start_time": "2024-10-03T22:14:06.833186608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "component_pos = {key: idx for idx, key in enumerate(df_normalised.columns)}\n",
    "#component_pos.items()"
   ],
   "id": "d374679f3925cbdc",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T22:14:06.871358327Z",
     "start_time": "2024-10-03T22:14:06.833291876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#data_array[component_pos['LIT101']]"
   ],
   "id": "ef6622de8b9b58c5",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T22:14:06.872254959Z",
     "start_time": "2024-10-03T22:14:06.834231059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#df_comp_files"
   ],
   "id": "f6ba77c089ac156e",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T22:14:06.924081455Z",
     "start_time": "2024-10-03T22:14:06.842547482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#df_comp_files['filename'] = '/home/loz/MSc/MSc_Project/final_code/component_objs/' + df_comp_files.index.astype(str)"
   ],
   "id": "c07ce45c9ecfaf57",
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "#df_comp_files.to_csv(path + 'component_filenames.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-03T22:14:06.924787817Z",
     "start_time": "2024-10-03T22:14:06.883303758Z"
    }
   },
   "id": "66acf0bf228d871"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T22:14:06.941597026Z",
     "start_time": "2024-10-03T22:14:06.883552738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#path = '/home/loz/Documents/GitHub/MSc-Project/final_code/component_objs/'\n",
    "path = '/home/loz/MSc/MSc_Project/final_code/component_objs/'\n",
    "\n",
    "\n",
    "df_comp_files = pd.read_csv(path + 'component_filenames.csv', index_col=0)\n",
    "components = {}\n",
    "for component_name, filename in df_comp_files['filename'].items():    \n",
    "    # put returned component object into dict\n",
    "    components[component_name] = load_component(filename)\n",
    "    #print(filename)"
   ],
   "id": "b36f327e1be69a9e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 'FIT101' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/FIT101.\n",
      "Component 'LIT101' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/LIT101.\n",
      "Component 'MV101' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/MV101.\n",
      "Component 'P101' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/P101.\n",
      "Component 'P102' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/P102.\n",
      "Component 'AIT201' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/AIT201.\n",
      "Component 'AIT202' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/AIT202.\n",
      "Component 'AIT203' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/AIT203.\n",
      "Component 'FIT201' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/FIT201.\n",
      "Component 'MV201' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/MV201.\n",
      "Component 'P201' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/P201.\n",
      "Component 'P202' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/P202.\n",
      "Component 'P203' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/P203.\n",
      "Component 'P204' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/P204.\n",
      "Component 'P205' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/P205.\n",
      "Component 'P206' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/P206.\n",
      "Component 'DPIT301' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/DPIT301.\n",
      "Component 'FIT301' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/FIT301.\n",
      "Component 'LIT301' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/LIT301.\n",
      "Component 'MV301' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/MV301.\n",
      "Component 'MV302' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/MV302.\n",
      "Component 'MV303' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/MV303.\n",
      "Component 'MV304' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/MV304.\n",
      "Component 'P301' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/P301.\n",
      "Component 'P302' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/P302.\n",
      "Component 'AIT401' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/AIT401.\n",
      "Component 'AIT402' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/AIT402.\n",
      "Component 'FIT401' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/FIT401.\n",
      "Component 'LIT401' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/LIT401.\n",
      "Component 'P401' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/P401.\n",
      "Component 'P402' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/P402.\n",
      "Component 'P403' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/P403.\n",
      "Component 'P404' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/P404.\n",
      "Component 'UV401' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/UV401.\n",
      "Component 'AIT501' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/AIT501.\n",
      "Component 'AIT502' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/AIT502.\n",
      "Component 'AIT503' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/AIT503.\n",
      "Component 'AIT504' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/AIT504.\n",
      "Component 'FIT501' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/FIT501.\n",
      "Component 'FIT502' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/FIT502.\n",
      "Component 'FIT503' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/FIT503.\n",
      "Component 'FIT504' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/FIT504.\n",
      "Component 'P501' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/P501.\n",
      "Component 'P502' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/P502.\n",
      "Component 'PIT501' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/PIT501.\n",
      "Component 'PIT502' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/PIT502.\n",
      "Component 'PIT503' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/PIT503.\n",
      "Component 'FIT601' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/FIT601.\n",
      "Component 'P601' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/P601.\n",
      "Component 'P602' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/P602.\n",
      "Component 'P603' has been loaded from /home/loz/MSc/MSc_Project/final_code/component_objs/P603.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T22:14:06.941805479Z",
     "start_time": "2024-10-03T22:14:06.894016262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "components['MV201'].correlation_dict"
   ],
   "id": "33f521277294fbe9",
   "outputs": [
    {
     "data": {
      "text/plain": "{'FIT101': 0.2302,\n 'LIT101': -0.6577,\n 'MV101': 0.2406,\n 'P101': 0.9897,\n 'AIT201': -0.0021,\n 'AIT202': -0.1859,\n 'AIT203': 0.0775,\n 'FIT201': 0.977,\n 'P203': 0.9848,\n 'P205': 0.5343,\n 'DPIT301': 0.5337,\n 'FIT301': 0.5394,\n 'LIT301': -0.4118,\n 'MV301': 0.0235,\n 'MV302': 0.4947,\n 'MV303': 0.0569,\n 'MV304': -0.0066,\n 'P301': 0.0065,\n 'P302': 0.5134,\n 'AIT401': 0.0157,\n 'AIT402': 0.1549,\n 'FIT401': 0.0932,\n 'LIT401': 0.3887,\n 'AIT501': -0.0611,\n 'AIT502': 0.2297,\n 'AIT503': 0.0196,\n 'AIT504': -0.0009,\n 'FIT501': 0.0896,\n 'FIT502': 0.0453,\n 'FIT503': -0.0266,\n 'FIT504': -0.0985,\n 'PIT501': 0.0511,\n 'PIT502': 0.0125,\n 'PIT503': 0.0516,\n 'FIT601': 0.0457,\n 'P602': 0.0423}"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "{'P402': 62.25151044819407,\n 'FIT504': 30.382519347111227,\n 'MV301': 23.38887956438467,\n 'PIT502': 23.2749610538029,\n 'AIT504': 20.813148267989725,\n 'FIT502': 18.814175364944887,\n 'P301': 15.100789862245776,\n 'MV303': 15.054578475734584,\n 'FIT401': 14.68496971690762,\n 'FIT501': 14.195996544548391,\n 'MV304': 13.791322257963998,\n 'PIT501': 12.387906291172175,\n 'FIT503': 10.482924424169877,\n 'AIT503': 10.213756008515617,\n 'P602': 9.0173072533069,\n 'AIT202': 8.55306520877577,\n 'LIT101': 6.994789177775087,\n 'AIT402': 6.933317612240199,\n 'AIT502': 6.512476306822948,\n 'AIT203': 6.32964093924371}"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "components['P203'].get_mutual_info(top_mi_components=20)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-03T22:14:06.960424463Z",
     "start_time": "2024-10-03T22:14:06.898401287Z"
    }
   },
   "id": "fdce06dc4296a69"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "{'FIT504': 249.8802513279516,\n 'P301': 119.48652422746956,\n 'FIT401': 85.78621238240744,\n 'FIT501': 82.11393949494311,\n 'PIT501': 47.03965856663376,\n 'P402': 46.28645294222533,\n 'FIT503': 45.91880163213426,\n 'AIT504': 38.69979186209629,\n 'AIT401': 36.10618720050128,\n 'MV301': 26.01056633010869,\n 'PIT502': 21.27328676324117,\n 'FIT502': 17.970737208955367,\n 'P602': 15.859431549576952,\n 'MV304': 13.20571692326176,\n 'FIT601': 12.196271074057226,\n 'AIT202': 11.635774857773724,\n 'MV303': 11.520720047081788,\n 'AIT502': 8.745994159607822,\n 'AIT402': 8.62724189219968,\n 'AIT203': 8.625869168760566,\n 'MV201': 7.436206489333187,\n 'P205': 6.998429635701775,\n 'P101': 6.997854986121927,\n 'P203': 6.994789177775077,\n 'FIT201': 6.969463016071852,\n 'MV302': 6.938716764824736,\n 'FIT101': 6.930097790159688,\n 'MV101': 6.460144496972306,\n 'P302': 6.179313766706848,\n 'FIT301': 5.978914732917646,\n 'DPIT301': 5.2769924592446085,\n 'AIT501': 5.1655791225537,\n 'LIT301': 5.11584469243656,\n 'LIT401': 5.014222422997274,\n 'AIT503': 3.374724743099263}"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "components['LIT101'].get_mutual_info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-03T22:14:06.960592109Z",
     "start_time": "2024-10-03T22:14:06.939417894Z"
    }
   },
   "id": "9650536df966d7ad"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-03T22:14:06.960645450Z",
     "start_time": "2024-10-03T22:14:06.939649531Z"
    }
   },
   "id": "7d0b9c36ae6d33cd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T22:14:07.671914821Z",
     "start_time": "2024-10-03T22:14:06.939766071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# test lstm swat simulator\n",
    "# Load the model from the SavedModel directory\n",
    "# 20 secs\n",
    "#swat_lstm = tf.keras.models.load_model('swat_lstm_1_0.keras')\n",
    "\n",
    "# get single output model\n",
    "swat_lstm = tf.keras.models.load_model('swat_lstm_lit101.keras')\n",
    "model_window = 20\n",
    "\n",
    "#swat_lstm = tf.keras.models.load_model('swat_lstm_100s.keras')\n",
    "#model_window = 100\n",
    "\n"
   ],
   "id": "c2384e58148b04d9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1727993646.936982  101554 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1727993646.937408  101554 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1727993646.937729  101554 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1727993647.006137  101554 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1727993647.006363  101554 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1727993647.006551  101554 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "prediction = swat_lstm.predict(reshaped_input)"
   ],
   "id": "3894c0718e36844b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Q Learning Table Implementation"
   ],
   "id": "3447e33dfb0856ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T22:14:07.704671533Z",
     "start_time": "2024-10-03T22:14:07.671123107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_array.shape"
   ],
   "id": "ad57c8d9f6dba87a",
   "outputs": [
    {
     "data": {
      "text/plain": "(86400, 22)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-03T22:16:57.673809971Z",
     "start_time": "2024-10-03T22:16:57.558906915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SwatEnv(Env):\n",
    "    def __init__(self, data_array, window_size=19, episode_length=100):\n",
    "        print('init reached')\n",
    "        \n",
    "        # Define the action space as a combination of 6 binary values (on/off states)\n",
    "        self.action_space = MultiBinary(6)  # Represents 6 binary actions: MV101, MV201, MV301, MV302, MV303, MV304\n",
    "        \n",
    "        # Define observation space (shape and limits can be adjusted as needed)\n",
    "        if len(data_array.shape) == 2:\n",
    "            # Add a batch dimension to make it (1, time_steps, features)\n",
    "            data_array = data_array[np.newaxis, :, :]\n",
    "        self.observation_space = Box(\n",
    "            low=np.round(np.full((1, 22), 0.00), 2),  # 1x22 array with all elements set to 0.00\n",
    "            high=np.round(np.full((1, 22), 100.00), 2),  # 1x22 array with all elements set to 100.00\n",
    "            dtype=np.float32  # Specify float with two decimal places precision\n",
    "        )\n",
    "        \n",
    "        # Initialize data parameters\n",
    "        self.data_array = data_array\n",
    "        self.window_size = window_size\n",
    "        self.episode_length = episode_length\n",
    "\n",
    "        # Use reset_env to initialize the data window to use\n",
    "        self.reset_env()      \n",
    "\n",
    "        # Initialize other variables\n",
    "        self.lit_value_record = {'LIT101': []}\n",
    "\n",
    "    def sample_observation(self):\n",
    "        # Return a random observation\n",
    "        return np.round(self.observation_space.sample(), 2)\n",
    "\n",
    "    def update_state(self, state_history, action_input):\n",
    "        # Ensure action_input is reshaped to 1x22 array\n",
    "        action_input = action_input.reshape(1, -1)\n",
    "\n",
    "        # Add action to previous states\n",
    "        model_input = np.concatenate((state_history[:, 1:, :], action_input[:, np.newaxis, :]), axis=1)\n",
    "        \n",
    "        # Predict the new state using the LSTM model (replace `swat_lstm` with your model)\n",
    "        new_state = swat_lstm.predict(model_input)\n",
    "        new_state = np.round(new_state, 2)\n",
    "        \n",
    "        # Flatten new_state from (1,22) to (22,)\n",
    "        #new_state = new_state.flatten()\n",
    "        \n",
    "        return new_state\n",
    "\n",
    "    def step(self, action):\n",
    "        # Copy the current state\n",
    "        action_input = self.state.copy()\n",
    "    \n",
    "        # Update the states of MV101, MV201, MV301, MV302, MV303, MV304 based on the binary action values\n",
    "        component_mapping = ['MV101', 'MV201', 'MV301', 'MV302', 'MV303', 'MV304']\n",
    "        for i, component in enumerate(component_mapping):\n",
    "            action_input[component_pos[component]] = action[i]\n",
    "    \n",
    "        # Get the new state using the updated action_input\n",
    "        new_state = self.update_state(self.state_history, action_input)\n",
    "    \n",
    "        # Initialize reward\n",
    "        reward = 0\n",
    "    \n",
    "        # Define learning rate and discount factor\n",
    "        alpha = 1  # Learning rate\n",
    "        gamma = 0.9  # Discount factor for future rewards\n",
    "        \n",
    "        total_change = 0\n",
    "        lit_values = []\n",
    "        # Calculate the reward for LIT101, LIT301, and LIT401 using Bellman equation concept\n",
    "        for lit_component in ['LIT101']:\n",
    "            lit_values.append((new_state, self.state[component_pos[lit_component]]))\n",
    "            # Calculate change in LIT value for the current component\n",
    "            change = new_state - self.state[component_pos[lit_component]]\n",
    "            self.lit_value_record[lit_component].append(change)\n",
    "            total_change += change\n",
    "    \n",
    "        # Calculate immediate reward based on change in value\n",
    "        immediate_reward = change\n",
    "        \n",
    "        # Estimate future reward using the discounted future value\n",
    "        future_reward = gamma * change\n",
    "\n",
    "        # Update the reward using a Bellman equation-like formula\n",
    "        reward += alpha * (immediate_reward + future_reward)\n",
    "        \n",
    "        print(f'reward, lit values:{reward}, {lit_values}')\n",
    "        # Update the environment state\n",
    "        self.state = new_state\n",
    "    \n",
    "        # Determine if the episode is done\n",
    "        done = True  # Single episode only, modify as needed\n",
    "    \n",
    "        return self.state, reward, self.lit_value_record, done\n",
    "\n",
    "\n",
    "    def reset_env(self):\n",
    "        # Ensure the data array has 3 dimensions (batch, time_steps, features)\n",
    "        if len(self.data_array.shape) == 2:\n",
    "            self.data_array = self.data_array[np.newaxis, :, :]\n",
    "\n",
    "        # Randomly select new window from data_array for each episode\n",
    "        start_idx = np.random.randint(0, self.data_array.shape[1] - self.window_size)\n",
    "        self.state_history = self.data_array[:, start_idx:start_idx + self.window_size, :]\n",
    "\n",
    "        # Set the start state as the last row in the selected window\n",
    "        self.state = self.state_history[0, -1, :]\n",
    "\n",
    "        # Reset value record for LIT101, LIT301, and LIT401\n",
    "        #self.lit_value_record = {'LIT101': [], 'LIT301': [], 'LIT401': []}\n",
    "        self.lit_value_record = {'LIT101': []}\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "    def reset(self):\n",
    "        # Override Gym's default reset method for compatibility\n",
    "        # Return initial observation based on the last reset_env state\n",
    "        return self.state\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        # Placeholder for rendering logic (e.g., using Pygame)\n",
    "        pass\n"
   ],
   "id": "57e0e505639a90cf",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T22:16:58.919551096Z",
     "start_time": "2024-10-03T22:16:58.876810878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bin_size = (1/50)\n",
    "bin_size"
   ],
   "id": "e6b52899d507994e",
   "outputs": [
    {
     "data": {
      "text/plain": "0.02"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-03T22:16:59.312235790Z",
     "start_time": "2024-10-03T22:16:59.239395174Z"
    }
   },
   "id": "a97ac0d03f7736dd"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "#component_pos"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-03T22:16:59.503193747Z",
     "start_time": "2024-10-03T22:16:59.458210586Z"
    }
   },
   "id": "d770e0467491a4d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T22:16:59.857836198Z",
     "start_time": "2024-10-03T22:16:59.786470685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "component_pos['LIT101']"
   ],
   "id": "a943ce0a01134498",
   "outputs": [
    {
     "data": {
      "text/plain": "21"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T22:17:00.218642982Z",
     "start_time": "2024-10-03T22:17:00.144243985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# q table at each 2000 episodes\n",
    "#q_table_list = []"
   ],
   "id": "14f72e87c9cd3c30",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T22:17:00.454264827Z",
     "start_time": "2024-10-03T22:17:00.387329884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#for _ in range(10):\n",
    "#    lit101_li, q_table = run_trial(SwatEnv,data_array,model_window,2000)\n",
    "#    q_table_list.append(q_table)"
   ],
   "id": "f42c0146cd2f032d",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T22:17:00.680976058Z",
     "start_time": "2024-10-03T22:17:00.607889448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#q_table_list\n"
   ],
   "id": "afd4772b95cc1e3f",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T22:17:01.022752171Z",
     "start_time": "2024-10-03T22:17:00.950476059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#np.savez('/home/loz/Documents/GitHub/MSc-Project/final_code#/component_objs/q_table_list_2000s.npz', *q_table_list) "
   ],
   "id": "6efcebbbaf98c704",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T22:17:01.226273284Z",
     "start_time": "2024-10-03T22:17:01.154289434Z"
    }
   },
   "cell_type": "code",
   "source": [],
   "id": "a4c40be9fa4bc99e",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Create Deep Policy Table"
   ],
   "id": "71582d3749d8b6e1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Create Deep Learning Model#"
   ],
   "id": "8cbe185c6d90d1ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T22:17:01.948024437Z",
     "start_time": "2024-10-03T22:17:01.897627812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Deep Q-Network (DQN) class for approximating Q-values\n",
    "class DQN:\n",
    "    def __init__(self, state_size, action_size, learning_rate=0.01):\n",
    "        self.state_size = state_size  # Number of features in the state\n",
    "        self.action_size = action_size  # Number of possible actions\n",
    "        self.learning_rate = learning_rate  # Learning rate for the neural network\n",
    "        \n",
    "        # Build the neural network model\n",
    "        self.model = self.build_model()\n",
    "        \n",
    "    def build_model(self):\n",
    "        # Define a neural network architecture\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Dense(64, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(layers.Dense(64, activation='relu'))\n",
    "        model.add(layers.Dense(self.action_size, activation='linear'))  # Output layer for Q-values\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate),\n",
    "                      loss='mse')\n",
    "        return model\n",
    "    \n",
    "    def predict(self, state):\n",
    "        # Predict Q-values for a given state\n",
    "        return self.model.predict(state, verbose=0)\n",
    "    \n",
    "    def train(self, state, target, batch_size=32):\n",
    "        # Train the network on a batch of data\n",
    "        self.model.fit(state, target, batch_size=batch_size, verbose=0)\n"
   ],
   "id": "6c984dd207cab06e",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T22:22:05.012997035Z",
     "start_time": "2024-10-03T22:22:04.909017275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import load_model\n",
    "\n",
    "def run_dqn_trial(environment, data_array, window_size, episodes, max_steps=100, batch_size=32, pretrained_model=None, save_path=None):\n",
    "    '''\n",
    "    Function to train or continue training a DQN agent with a given starting point of a data window from the data_set.\n",
    "    :param environment: SWaTEnv environment.\n",
    "    :param data_array: Input data array for the environment.\n",
    "    :param window_size: Size of the data window for state history.\n",
    "    :param episodes: Number of episodes for training.\n",
    "    :param max_steps: Maximum steps per episode.\n",
    "    :param batch_size: Batch size for training the neural network.\n",
    "    :param pretrained_model: Optional. If provided, the function will use this pre-trained model to continue training.\n",
    "    :param save_path: Optional. Path to save the trained model after training.\n",
    "    :return: List of recorded LIT101 values, rewards, losses, and trained model.\n",
    "    '''\n",
    "\n",
    "    # Initialize the environment\n",
    "    this_env = environment(data_array, window_size)\n",
    "    \n",
    "    # Get state and action sizes\n",
    "    state_size = this_env.observation_space.shape[1]  #  observation space is (1, 22)\n",
    "    action_size = this_env.action_space.n  # Number of possible actions (should be 6 for MultiBinary MV components)\n",
    "\n",
    "    # Initialize or load the DQN model\n",
    "    if pretrained_model:\n",
    "        print(\"Loading pre-trained model...\")\n",
    "        dqn_model = pretrained_model  # Use the provided pre-trained model\n",
    "    else:\n",
    "        print(\"Initializing new model...\")\n",
    "        dqn_model = DQN(state_size, action_size)  # Create a new DQN model if no pre-trained model is provided\n",
    "    \n",
    "    # Hyperparameters\n",
    "    gamma = 0.9  # Discount factor\n",
    "    alpha = 1.0  # Learning rate for Q-value updates (for Bellman equation)\n",
    "    epsilon = 1.0 if not pretrained_model else 0.1  # Exploration rate (lower for pretrained models)\n",
    "    epsilon_decay = 0.99  # Decay rate for exploration\n",
    "    min_epsilon = 0.01  # Minimum exploration rate\n",
    "    \n",
    "    # Replay buffer for experience replay\n",
    "    replay_buffer = deque(maxlen=2000)  # You can increase the size if needed\n",
    "    \n",
    "    lit101_values = []  # List to store LIT101 values across episodes\n",
    "    episode_rewards = []  # Store total rewards per episode\n",
    "    losses = []  # Store losses for visualization\n",
    "\n",
    "    for episode in range(1, episodes + 1):\n",
    "        # Reset the environment and get the initial state by randomly selecting window of data\n",
    "        state = this_env.reset()  \n",
    "        \n",
    "        state = np.reshape(state, [1, state_size])  # Reshape state for the neural network\n",
    "\n",
    "        episode_reward = 0\n",
    "        loss = 0  # Initialize loss for the episode to ensure it has a value\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # Choose action using epsilon-greedy policy\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = this_env.action_space.sample()  # Explore: select a random action\n",
    "            else:\n",
    "                q_values = dqn_model.predict(state)\n",
    "                action_index = np.argmax(q_values)  # Exploit: select action with max Q-value\n",
    "\n",
    "                # Convert action index to binary array of length 6 (e.g., 10 -> [0, 0, 1, 0, 1, 0])\n",
    "                action = np.array(list(np.binary_repr(action_index, width=6))).astype(int)\n",
    "\n",
    "            # Take the action and observe the new state and reward\n",
    "            next_state, reward, value_change, done = this_env.step(action)\n",
    "            \n",
    "            # no reshape, single value\n",
    "            #next_state = np.reshape(next_state, [1, state_size])  # Reshape for neural network\n",
    "            \n",
    "            # Store experience in replay buffer\n",
    "            replay_buffer.append((state, action, reward, next_state, done))\n",
    "            \n",
    "            # Update the current state\n",
    "            #state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            # If the episode is finished, exit the loop\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            # Train the neural network with a random batch from the replay buffer\n",
    "            if len(replay_buffer) > batch_size:\n",
    "                minibatch = random.sample(replay_buffer, batch_size)\n",
    "                states = np.array([experience[0] for experience in minibatch]).reshape(batch_size, state_size)\n",
    "                actions = np.array([experience[1] for experience in minibatch])\n",
    "                rewards = np.array([experience[2] for experience in minibatch])\n",
    "                next_states = np.array([experience[3] for experience in minibatch]).reshape(batch_size, state_size)\n",
    "                dones = np.array([experience[4] for experience in minibatch])\n",
    "\n",
    "                # Predict Q-values for next states\n",
    "                target_q_values = dqn_model.predict(next_states)\n",
    "                \n",
    "                # Calculate target values using Bellman equation\n",
    "                targets = dqn_model.predict(states)\n",
    "                for i in range(batch_size):\n",
    "                    action_index = np.argmax(actions[i])  # Get the index of the chosen action\n",
    "                    if dones[i]:\n",
    "                        targets[i][action_index] = rewards[i]  # If terminal state, just use reward\n",
    "                    else:\n",
    "                        # Bellman equation: Q(s, a) = r + gamma * max(Q(s', a'))\n",
    "                        targets[i][action_index] = rewards[i] + gamma * np.max(target_q_values[i])\n",
    "\n",
    "                # Train the model on the updated Q-values and capture loss\n",
    "                loss = dqn_model.model.train_on_batch(states, targets)  # Use train_on_batch to get the loss\n",
    "                losses.append(loss)\n",
    "\n",
    "        # Append LIT101 value changes for tracking\n",
    "        lit101_values.append(this_env.lit_value_record)\n",
    "        episode_rewards.append(episode_reward)\n",
    "        \n",
    "        # Reduce exploration rate\n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "        \n",
    "        # Print episode statistics, with `loss` always defined\n",
    "        print(f'Episode: {episode}/{episodes}, Reward: {episode_reward}, Epsilon: {epsilon:.4f}, Loss: {loss:.4f}')\n",
    "        \n",
    "    # Save the trained model if a save path is provided\n",
    "    if save_path:\n",
    "        dqn_model.model.save(save_path)\n",
    "        print(f\"Model saved to {save_path}\")\n",
    "\n",
    "    # Return collected metrics and trained model\n",
    "    return lit101_values, episode_rewards, losses, dqn_model\n"
   ],
   "id": "46bc39812daedfd8",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T22:22:05.866905470Z",
     "start_time": "2024-10-03T22:22:05.751255559Z"
    }
   },
   "cell_type": "code",
   "source": [],
   "id": "124a2946ba5c8e32",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T22:22:06.046663105Z",
     "start_time": "2024-10-03T22:22:06.001748977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#states = env.observation_space.shape\n",
    "#actions = env.action_space.n"
   ],
   "id": "803ec7a6019f3243",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T22:22:06.299898166Z",
     "start_time": "2024-10-03T22:22:06.231549749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "episode_rewards"
   ],
   "id": "3d92f5387e843d6d",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'episode_rewards' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[51], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mepisode_rewards\u001B[49m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'episode_rewards' is not defined"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T22:22:06.654306290Z",
     "start_time": "2024-10-03T22:22:06.545820799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lit101_values"
   ],
   "id": "b8127ff94cf4b46d",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lit101_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[52], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mlit101_values\u001B[49m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'lit101_values' is not defined"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T22:22:07.033617484Z",
     "start_time": "2024-10-03T22:22:06.882271468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load an existing model or set it to None to train from scratch\n",
    "#pretrained_model_path = 'pretrained_dqn_model.h5'\n",
    "#pretrained_model = load_model(pretrained_model_path) if os.path.exists(pretrained_model_path) else None\n",
    "\n",
    "# Train or continue training the model\n",
    "lit101_values, episode_rewards, losses, trained_model = run_dqn_trial(\n",
    "    SwatEnv, data_array, window_size=19, episodes=1000, pretrained_model=None, save_path='improved_dqn_model.keras'\n",
    ")\n"
   ],
   "id": "d935051c59e27f38",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init reached\n",
      "Initializing new model...\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step\n",
      "reward, lit values:[[0.1033365]], [(array([[1.04]], dtype=float32), 0.9856123171360828)]\n",
      "Episode: 1/1000, Reward: [[0.1033365]], Epsilon: 0.9900, Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loz/MSc/MSc_Project/venv/lib/python3.10/site-packages/gym/spaces/box.py:127: UserWarning: \u001B[33mWARN: Box bound precision lowered by casting to float32\u001B[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "/home/loz/MSc/MSc_Project/venv/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1 into shape (1,22)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[53], line 6\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Load an existing model or set it to None to train from scratch\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#pretrained_model_path = 'pretrained_dqn_model.h5'\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m#pretrained_model = load_model(pretrained_model_path) if os.path.exists(pretrained_model_path) else None\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Train or continue training the model\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m lit101_values, episode_rewards, losses, trained_model \u001B[38;5;241m=\u001B[39m \u001B[43mrun_dqn_trial\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mSwatEnv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_array\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwindow_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m19\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepisodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpretrained_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msave_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mimproved_dqn_model.keras\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\n\u001B[1;32m      8\u001B[0m \u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[49], line 52\u001B[0m, in \u001B[0;36mrun_dqn_trial\u001B[0;34m(environment, data_array, window_size, episodes, max_steps, batch_size, pretrained_model, save_path)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m episode \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, episodes \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;66;03m# Reset the environment and get the initial state\u001B[39;00m\n\u001B[1;32m     51\u001B[0m     state \u001B[38;5;241m=\u001B[39m this_env\u001B[38;5;241m.\u001B[39mreset()  \u001B[38;5;66;03m# Use the environment's reset method without additional arguments\u001B[39;00m\n\u001B[0;32m---> 52\u001B[0m     state \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate_size\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Reshape state for the neural network\u001B[39;00m\n\u001B[1;32m     54\u001B[0m     episode_reward \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     55\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m  \u001B[38;5;66;03m# Initialize loss for the episode to ensure it has a value\u001B[39;00m\n",
      "File \u001B[0;32m~/MSc/MSc_Project/venv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:285\u001B[0m, in \u001B[0;36mreshape\u001B[0;34m(a, newshape, order)\u001B[0m\n\u001B[1;32m    200\u001B[0m \u001B[38;5;129m@array_function_dispatch\u001B[39m(_reshape_dispatcher)\n\u001B[1;32m    201\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreshape\u001B[39m(a, newshape, order\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mC\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m    202\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    203\u001B[0m \u001B[38;5;124;03m    Gives a new shape to an array without changing its data.\u001B[39;00m\n\u001B[1;32m    204\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    283\u001B[0m \u001B[38;5;124;03m           [5, 6]])\u001B[39;00m\n\u001B[1;32m    284\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 285\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_wrapfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mreshape\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnewshape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43morder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43morder\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/MSc/MSc_Project/venv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59\u001B[0m, in \u001B[0;36m_wrapfunc\u001B[0;34m(obj, method, *args, **kwds)\u001B[0m\n\u001B[1;32m     56\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _wrapit(obj, method, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mbound\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001B[39;00m\n\u001B[1;32m     62\u001B[0m     \u001B[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     66\u001B[0m     \u001B[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001B[39;00m\n\u001B[1;32m     67\u001B[0m     \u001B[38;5;66;03m# exception has a traceback chain.\u001B[39;00m\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _wrapit(obj, method, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n",
      "\u001B[0;31mValueError\u001B[0m: cannot reshape array of size 1 into shape (1,22)"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-03T22:14:07.747412745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assuming `lit101_values`, `episode_rewards`, `losses`, and `trained_model` are obtained from training:\n",
    "#lit101_values, episode_rewards, losses, trained_model = run_dqn_trial(SwatEnv, data_array, #window_size=19, episodes=200)\n",
    "\n",
    "# Test the trained model's performance\n",
    "test_model_performance(SwatEnv, trained_model, data_array, window_size=19, test_episodes=10, max_steps=200)\n"
   ],
   "id": "6ba60cf6161d3e4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T22:14:07.956082290Z",
     "start_time": "2024-10-03T22:14:07.747461547Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def test_model_performance(environment, model, data_array, window_size=19, test_episodes=10, max_steps=100):\n",
    "    \"\"\"\n",
    "    Test the performance of the trained DQN model.\n",
    "    \n",
    "    :param environment: SWaTEnv environment.\n",
    "    :param model: Trained DQN model.\n",
    "    :param data_array: Input data array for the environment.\n",
    "    :param window_size: Size of the data window for state history.\n",
    "    :param test_episodes: Number of test episodes to evaluate.\n",
    "    :param max_steps: Maximum steps per episode.\n",
    "    :return: None. The function prints and plots the performance results.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the environment for testing\n",
    "    test_env = environment(data_array, window_size)\n",
    "    \n",
    "    # Metrics to track\n",
    "    total_rewards = []\n",
    "    lit_values_changes = {'LIT101': [], 'LIT301': [], 'LIT401': []}  # Track LIT values changes over episodes\n",
    "\n",
    "    for episode in range(1, test_episodes + 1):\n",
    "        # Reset the environment to get the initial state\n",
    "        state = test_env.reset()\n",
    "        state = np.reshape(state, [1, test_env.observation_space.shape[1]])  # Reshape for the neural network\n",
    "\n",
    "        episode_reward = 0\n",
    "        lit101_values = []  # Track LIT101 values for the episode\n",
    "        lit301_values = []  # Track LIT301 values for the episode\n",
    "        lit401_values = []  # Track LIT401 values for the episode\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # Select action based on the highest Q-value (no exploration)\n",
    "            q_values = model.predict(state)\n",
    "            action_index = np.argmax(q_values)\n",
    "\n",
    "            # Convert action index to binary array of length 6\n",
    "            action = np.array(list(np.binary_repr(action_index, width=6))).astype(int)\n",
    "\n",
    "            # Take the action and observe the next state and reward\n",
    "            next_state, reward, value_change, done = test_env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, test_env.observation_space.shape[1]])  # Reshape for the network\n",
    "\n",
    "            # Accumulate the reward\n",
    "            episode_reward += reward\n",
    "            lit101_values.extend(value_change['LIT101'])  # Track LIT101 value changes\n",
    "            lit301_values.extend(value_change['LIT301'])  # Track LIT301 value changes\n",
    "            lit401_values.extend(value_change['LIT401'])  # Track LIT401 value changes\n",
    "\n",
    "            # Update the current state\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Track metrics for this episode\n",
    "        total_rewards.append(episode_reward)\n",
    "        lit_values_changes['LIT101'].append(lit101_values)\n",
    "        lit_values_changes['LIT301'].append(lit301_values)\n",
    "        lit_values_changes['LIT401'].append(lit401_values)\n",
    "\n",
    "        print(f\"Test Episode: {episode}/{test_episodes}, Total Reward: {episode_reward:.4f}\")\n",
    "\n",
    "    # Print average performance\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    print(f\"Average Total Reward over {test_episodes} episodes: {avg_reward:.4f}\")\n",
    "\n",
    "    # Plot performance results\n",
    "    plot_test_performance(total_rewards, lit_values_changes)\n",
    "\n",
    "\n",
    "def plot_test_performance(rewards, lit_values_changes, rolling_window=10):\n",
    "    \"\"\"\n",
    "    Plot the rewards and rolling average of LIT values over test episodes.\n",
    "\n",
    "    :param rewards: List of rewards per episode.\n",
    "    :param lit_values_changes: Dictionary of LIT value changes over episodes.\n",
    "    :param rolling_window: Window size for calculating rolling average.\n",
    "    :return: None.\n",
    "    \"\"\"\n",
    "    # Create subplots for total rewards and rolling average for LIT values\n",
    "    plt.figure(figsize=(15, 8))\n",
    "\n",
    "    # Plot total rewards for each episode\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(range(1, len(rewards) + 1), rewards, marker='o')\n",
    "    plt.title('Total Rewards per Test Episode')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Create a subplot for rolling average of LIT101, LIT301, and LIT401 changes\n",
    "    plt.subplot(2, 1, 2)\n",
    "    for lit_component, value_changes in lit_values_changes.items():\n",
    "        # Flatten the list of lists for each component\n",
    "        flattened_values = [item for sublist in value_changes for item in sublist]\n",
    "        \n",
    "        # Calculate rolling average\n",
    "        if len(flattened_values) >= rolling_window:\n",
    "            rolling_avg = pd.Series(flattened_values).rolling(window=rolling_window).mean()\n",
    "            plt.plot(rolling_avg, label=f'{lit_component} Rolling Average')\n",
    "\n",
    "    plt.title('Rolling Average of LIT Value Changes')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Rolling Average Change')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "id": "69758ce59d7ad830"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-03T22:14:07.747511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.summary()"
   ],
   "id": "83d8647280b71a53",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def predict_and_plot_lit301(lstm_model, data_array, window_size=19, num_steps=1200):\n",
    "    \"\"\"\n",
    "    Predict 1200 consecutive steps using the LSTM model and plot LIT301 against original values.\n",
    "    \n",
    "    :param lstm_model: The trained LSTM model.\n",
    "    :param data_array: The original data array used for training.\n",
    "    :param window_size: The size of the input window for predictions.\n",
    "    :param num_steps: The number of consecutive steps to predict.\n",
    "    :return: None. The function generates a plot.\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(data_array.shape) == 2:\n",
    "        data_array = data_array[np.newaxis, :, :]  # Adds a new axis to make it 3D\n",
    "    # Select the initial window of data for prediction\n",
    "    state_history = data_array[:, :window_size, :]\n",
    "    \n",
    "    # Placeholder for predicted LIT301 values\n",
    "    predicted_lit301 = []\n",
    "    original_lit301 = []\n",
    "\n",
    "    # Make sequential predictions for `num_steps` steps\n",
    "    for step in range(num_steps):\n",
    "        # Predict the next state using the LSTM model\n",
    "        next_state = lstm_model.predict(state_history)\n",
    "        \n",
    "        # Append the LIT301 value from the predicted state and the original state\n",
    "        predicted_lit301.append(next_state[0, component_pos['LIT301']])\n",
    "        original_lit301.append(data_array[0, window_size + step, component_pos['LIT301']])\n",
    "        \n",
    "        # Update the state history by appending the new state and removing the oldest state\n",
    "        state_history = np.concatenate((state_history[:, 1:, :], next_state[:, np.newaxis, :]), axis=1)\n",
    "        \n",
    "        # Stop if we reach the end of the available data\n",
    "        if window_size + step + 1 >= data_array.shape[1]:\n",
    "            break\n",
    "\n",
    "    # Convert lists to numpy arrays for easy plotting\n",
    "    predicted_lit301 = np.array(predicted_lit301)\n",
    "    original_lit301 = np.array(original_lit301)\n",
    "\n",
    "    # Plot the predicted LIT301 values against the original values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(range(len(original_lit301)), original_lit301, label='Original LIT301', color='blue')\n",
    "    plt.plot(range(len(predicted_lit301)), predicted_lit301, label='Predicted LIT301', linestyle='--', color='red')\n",
    "    plt.title('LIT301: Original vs. Predicted Values over 1200 Steps')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('LIT301 Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-10-03T22:14:07.747559592Z"
    }
   },
   "id": "478f1bc17bca7a69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-03T22:14:07.747649952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assuming `lstm_model` is your trained LSTM model and `data_array` is your input data\n",
    "predict_and_plot_lit301(swat_lstm, data_array, window_size=19, num_steps=1200)\n"
   ],
   "id": "b03b85a483592efc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-03T22:14:07.747776902Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [],
   "id": "bc4c45a9b8326aba"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
